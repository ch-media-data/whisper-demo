{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWKWKCNeaJD8"
      },
      "source": [
        "# OpenAI's Whisper Speech Recognition Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-kqGN7aJD9"
      },
      "source": [
        "![alt text](whisper.jpg \"Intro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbja1jB3vDOT"
      },
      "source": [
        "\n",
        "Whisper ist der Name eines automatischen Spracherkennungssystems (ASR) von OpenAI. ASR-Systeme wandeln gesprochene Sprache in geschriebenen Text um. Sie werden in einer Vielzahl von Anwendungen eingesetzt, darunter Sprachassistenten, Transkriptionsdienste und mehr.\n",
        "\n",
        "Whisper wurde auf einer großen Menge multilingualer und multitaskaler Daten aus dem Web trainiert. Es wurde entwickelt, um in verschiedenen Anwendungen und Kontexten eine hohe Genauigkeit bei der Spracherkennung zu bieten.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kosakhNmxb7A"
      },
      "source": [
        "## Install Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsJUxc0aRsAf",
        "outputId": "9399561e-a294-4d68-980c-68829d3327b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=b1ec1da95c0c1d6663a27a50c7d2b9b5d4f76130199ef79ebb3138f9c51c6a5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i3bKhnZaJD-"
      },
      "source": [
        "# Nur ausführen wenn ihr auf einem Colab notebook seid\n",
        "- dies lädt euch die mp3 dateien runter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VMqa3KAEaJD-",
        "outputId": "9348037c-45af-44cf-a09c-56967def9995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 13:41:03--  https://github.com/MAZ-CAS-DDJ/kurs_23_24/raw/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/mary.mp3\n",
            "Resolving github.com (github.com)... 20.29.134.23\n",
            "Connecting to github.com (github.com)|20.29.134.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MAZ-CAS-DDJ/kurs_23_24/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/mary.mp3 [following]\n",
            "--2023-11-23 13:41:03--  https://raw.githubusercontent.com/MAZ-CAS-DDJ/kurs_23_24/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/mary.mp3\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 100483 (98K) [audio/mpeg]\n",
            "Saving to: ‘mary.mp3’\n",
            "\n",
            "mary.mp3            100%[===================>]  98.13K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-11-23 13:41:03 (6.07 MB/s) - ‘mary.mp3’ saved [100483/100483]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MAZ-CAS-DDJ/kurs_23_24/raw/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/mary.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-EcHdr5BaJD-",
        "outputId": "c0011848-c90a-4aa2-a45e-ef05f41708d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-23 13:41:06--  https://github.com/MAZ-CAS-DDJ/kurs_23_24/raw/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/daisy_HAL_9000.mp3\n",
            "Resolving github.com (github.com)... 20.29.134.23\n",
            "Connecting to github.com (github.com)|20.29.134.23|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MAZ-CAS-DDJ/kurs_23_24/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/daisy_HAL_9000.mp3 [following]\n",
            "--2023-11-23 13:41:06--  https://raw.githubusercontent.com/MAZ-CAS-DDJ/kurs_23_24/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/daisy_HAL_9000.mp3\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150436 (147K) [audio/mpeg]\n",
            "Saving to: ‘daisy_HAL_9000.mp3’\n",
            "\n",
            "daisy_HAL_9000.mp3  100%[===================>] 146.91K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-11-23 13:41:06 (6.46 MB/s) - ‘daisy_HAL_9000.mp3’ saved [150436/150436]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MAZ-CAS-DDJ/kurs_23_24/raw/main/13%20Textanalyse%20Verfiefung%20Teil%201%20und%202/daisy_HAL_9000.mp3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuj6PzO8aJD-"
      },
      "source": [
        "Ändert die Laufzeit auf CPU, sonst beschwert sich Colab dass ich beim Free-Tier der GPU ram nicht reicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MpJtE31aJD_"
      },
      "source": [
        "![alt text](laufzeit.png \"Intro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtAvuKSJxhNw"
      },
      "source": [
        "## Modell laden\n",
        "\n",
        "- Das sollte ca. 140 MB runterladen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr5faKybKi4p",
        "outputId": "6eac3fdc-3c2f-4cab-86ea-f24bb2d3afb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 103MiB/s]\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import numpy as np\n",
        "model = whisper.load_model(\"base\") # oder tiny falls sich google colab beschwert dass der ram nicht reicht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e200RNNlxn5j"
      },
      "source": [
        "## Können wir die GPU nutzen?\n",
        "\n",
        "Auf der GPU vs. CPU arbeiten. Im besten Fall steht dort device GPU, das hängt von eurem Laptop ab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_6_s2iHboR4",
        "outputId": "85ebf5e9-f503-4ee3-fade-6011a6d7b4c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwLTZtubySoo"
      },
      "source": [
        "## Define the Transcribe Function\n",
        "\n",
        "Jetzt haben wir das Modell geladen und haben den Code. Dies ist die Funktion, die einen Audiodateipfad als Eingabe verwendet und den erkannten Text zurückgibt (und protokolliert, was seiner Meinung nach die Sprache ist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QRwhuOXynZQ"
      },
      "source": [
        "## Test mit Low und High Quality Audio\n",
        "\n",
        "Ihr werdet feststellen, dass das zweite Transkript normalerweise nach 30 Sekunden abgeschnitten wird. Dies ist die Standardlänge. Diese kann erweitert werden. Siehe oben, mit chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDXgLIprIsAj",
        "outputId": "257bc124-cf7b-47c4-f61c-f224e1d2bd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mary had a little lamb, its fleece was white as snow, and everywhere that Mary went, the lamb was sure to go.\n",
            " Tazy, Tazy, Tazy. Give me your answer to time after crazy all for the love of you. It won't be a stylish marriage like time or for the love of others. For the love of others. For the love of others.\n"
          ]
        }
      ],
      "source": [
        "easy_text = model.transcribe(\"mary.mp3\")\n",
        "print(easy_text[\"text\"])\n",
        "\n",
        "hard_text = model.transcribe(\"daisy_HAL_9000.mp3\")\n",
        "print(hard_text[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "choAtF-aaJEH"
      },
      "source": [
        "# Download a youtube video, transcode it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rzkjItSRaJEH",
        "outputId": "620a00e5-b462-4b2b-c17f-a8efa9a3999b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Requirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.4)\n",
            "Installing collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.19.0 websockets-12.0 yt-dlp-2023.11.16\n"
          ]
        }
      ],
      "source": [
        "!pip install -U yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jGpRRxL5aJEH",
        "outputId": "ebdfd17c-c198-4a84-9928-6478e69f0760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=ww0zvl18-Ys\n",
            "[youtube] ww0zvl18-Ys: Downloading webpage\n",
            "[youtube] ww0zvl18-Ys: Downloading ios player API JSON\n",
            "[youtube] ww0zvl18-Ys: Downloading android player API JSON\n",
            "[youtube] ww0zvl18-Ys: Downloading m3u8 information\n",
            "[info] ww0zvl18-Ys: Downloading 1 format(s): 247+251\n",
            "[download] Destination: Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].f247.webm\n",
            "\u001b[K[download] 100% of    7.33MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m13.50MiB/s\u001b[0m\n",
            "[download] Destination: Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].f251.webm\n",
            "\u001b[K[download] 100% of    3.30MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m9.68MiB/s\u001b[0m\n",
            "[Merger] Merging formats into \"Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].webm\"\n",
            "Deleting original file Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].f247.webm (pass -k to keep)\n",
            "Deleting original file Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].f251.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "!yt-dlp \"https://www.youtube.com/watch?v=ww0zvl18-Ys\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lNYimvj9aJEI"
      },
      "outputs": [],
      "source": [
        "charlie = model.transcribe(\"Charlie Chaplins Rede an die Menschheit [ww0zvl18-Ys].webm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WASePfFeaJEI",
        "outputId": "135cc6d1-21b5-4170-bf63-4fd345b66d4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Es tut mir leid, aber ich möchte nun mal kein Herrscher der Welt sein, denn das liegt mir nicht. Ich möchte weder Herrchen noch irgendwie erobern, sondern jede Menschen helfen, wo immer ich kann, den Juden, den Heiden, den Fahrwigen, den Weißen. Jeder Mensch sollte dem anderen helfen, nur so verbessern wir die Welt. Wir sollten am Glück des anderen Teilhaben und nicht an der Verabschollen. Pass und Verachtung bringen uns niemals näher. Auf dieser Welt ist Platz genug für jeden und Mutter Erde ist reich genug, um jeden von uns Satz zu machen. Das Leben kann ja so erfreulich und wunderbar sein. Wir müssen es nur wieder zu leben lernen. Die Habki hat das Gute im Menschen verschützt und Misskunst hat die Seelen vergiftet und uns im Parade-Schritt zu verdärb und Blutschuld geführt. Wir haben die Geschwindigkeit entwickelt, aber innerlich sind wir stehen geblieben. Wir lassen Maschinen für uns arbeiten und sie denken auch für uns. Die Glugheit hat uns hochmütig werden lassen und unser Wissen kalt und hart. Wir sprechen zu viel und fühlen zu wenig, aber zuerst kommt die Menschlichkeit und dann erst die Maschinen. Vorglugheit und Wissen kommt Toleranz und Gute. Wo eine Menschlichkeit und Nächsten Liebe ist, unser Dasein nicht lebenswert. Eroplane und Radio haben uns ein Andernäher gebracht. Die sehr Findungen haben eine Brücke geschlagen von Mensch zu Mensch. Die erforderne eine allumfassende Brüderlichkeit, damit wir alle einswerten. Millionen Menschen auf der Welt können im Augenblick meine Stimme hören. Millionen verzweifelter Menschen. Ob für eine System, dass es sich zur Aufgabe gemacht hat, unschuldige zu quälen und im Ketten zu legen. Alle denen, die mich jetzt hören, ruf ich zu, ihr dürft nicht verzagen. Auch das Bitteleid, dass über uns gekommen ist, ist vergänglich. Die Männer, die heute die Menschlichkeit mit Füßen treten, werden nicht immer da sein. Ihre Grausamkeit stirbt mit ihnen und auch ihr Hass. Die Freiheit, die sie den Menschen genommen haben, wird ihnen dann zu Höcke geben werden. Auch wenn es Blut und Reinen kostet. Für die Freiheit ist kein Opfer zu groß. Soll da, vertraut euch nicht barbaren. An unmenschen die euch verachten und denen euer Leben nichts wert ist. Ihr seid für sie nur Sklaven. Ihr habt das zu tun, das zu glauben, das zu fühlen. Ihr werdet jetzt erhält gefüttert, wie viel behandelt und seid nichts weiter als Kanonen vor der. Ihr seid viel zu schade für diese verehrten Subjekte. Diese Maschine, Menschen mit Maschinen kürzen und Maschinen herzen. Ihr seid keine Roboter, ihr seid keine Tiere, ihr seid Menschen. Er wartet euch die Menschlichkeit in euren Herzen und hast nicht. Nur wenn nicht geliebt wird, hast, nur wenn nicht geliebt wird. Soll da, kämpft nicht für die Sklaverein, kämpft für die Freiheit. Im 17. Kapitel, das Evangelisten Lukas steht Gott, wohnt in jede Mensch. Also nicht nur in einem, wo eine Gruppe von Menschen perck, der Sneegop bliebt in euch allen und ihr als Volkab allein die Macht. Ihr macht Kanonen zu verbritzieren, aber auch die Macht glück zu spät. Ihr als Volkab, das in der Hand dieses Leben einmalig kostbar zu machen. Es wird wunderbar im Freiheitsgeist zu den Tringen. Da ihr im Namen der Demokratie, lasst uns diese Macht nutzen. Lasst uns zusammenstehen. Lasst uns kämpfen für eine neue Welt, für eine Anständige Welt. Wie jeder Mann gleiche schossen gibt, wie der Jugend in eine Zukunft und in alten Sicherheit gewährt. Er schrafen abend Jörgertäuker das auch. Deshalb kommt sie die Macht da rein. Das war überall. Alles was ihr euch versparen, diese Paperecher. Diktatorin wollen die Freiheit nur für sich. Das Volk soll was verbleiben. Lasst uns diese Kenntnisch einigen. Lasst uns kämpfen für eine bessere Welt. Lasst uns kämpfen für die Freiheit in der Welt. Was ist ein Ziel, für das sich zu kämpfen lohnt? Nieder mit der Unterkunft im Hass und der Intoleran. Lasst uns kämpfen für eine Welt, das sauberkeit. In der die Vernunft zieht, in der Olbschöp und Wissenschaft uns alle zu sehen. Kann man ja in Namen der Demokratie, dafür lasst uns steigen.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "charlie[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wAUzBdoaJEI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}